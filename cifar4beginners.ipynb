{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar4beginners.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnLCqr3tLiyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Code written by Dweep Joshipura\n",
        "Blog - lightspeedac.blogspot.com\n",
        "Please read the comments carefully\n",
        "'''\n",
        "# Please use the GPU Hardware Accelerator to save time.\n",
        "# Go to Edit->Notebook Settings->Hardware Accelerator and choose GPU and Save\n",
        "# Importing keras indirectly\n",
        "import tensorflow.keras as keras\n",
        "# Importing kinds of layers\n",
        "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,BatchNormalization,Dropout,Flatten\n",
        "# Importing basic sequential model\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Importing 10 class classifying dataset cifar10\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "# Importing preprocessing function\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Ensuring TF 1 is used\n",
        "%tensorflow_version 1.x\n",
        "# Getting data from cifar10\n",
        "'''\n",
        "x is the Input\n",
        "y is the Truth Output for the Input x\n",
        "x_test is additional testing input\n",
        "y_test is Truth Output for Input X_test\n",
        "'''\n",
        "(x,y),(x_test,y_test) = cifar10.load_data()\n",
        "# We have 10 classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "num_classes = 10\n",
        "# Preprocessing the ground truth\n",
        "y = to_categorical(y,num_classes)\n",
        "y_test = to_categorical(y_test,num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXxdAmxRN2eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This model also uses the Sequential api.\n",
        "model = Sequential(name=\"My-first-CNN\")\n",
        "'''\n",
        "We are applying the convolutional operation to the input.\n",
        "3X3 filters are used (f=3)\n",
        "padding=\"same\" ensures that the image isn't shrunk (p=\"same\")\n",
        "We are applying the 'relu' activation talked about in Intro to DL blog\n",
        "We are applying 32 such filters(small matrices) (nf = 32)\n",
        "'''\n",
        "model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\",activation='relu',input_shape=x.shape[1:]))\n",
        "'''\n",
        "nf = 64\n",
        "f = 5\n",
        "p = \"valid\"\n",
        "You may edit and play around to maximize accuracy\n",
        "'''\n",
        "model.add(Conv2D(filters=64,kernel_size=(5,5),activation='relu'))\n",
        "'''\n",
        "This is another type of layer that simply takes 2-by-2 chunks and selects the Maximum value\n",
        "The size = 2 is default in Keras\n",
        "It halves the size and will be referred to as a 'halving' layer\n",
        "'''\n",
        "model.add(MaxPooling2D())\n",
        "'''\n",
        "The third type of layer is called Batch Normalization\n",
        "This normalizes the previous layer output\n",
        "'''\n",
        "model.add(BatchNormalization())\n",
        "#nf = 32, f = 3,p = \"valid\"\n",
        "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu'))\n",
        "# Another 'halving' layer\n",
        "model.add(MaxPooling2D())\n",
        "# Normalizing layer\n",
        "model.add(BatchNormalization())\n",
        "# A dropout layer as talked about in intro to Deep Learning CODE.\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# We encourage you to play around with parameters like dropout rate and f,p,nf\n",
        "# nf = 64, f = 3, p = \"same\" (We will not write these values, we are assuming you understand where they're written)\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\",activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128,kernel_size=(7,7),activation='relu',padding=\"same\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding=\"same\"))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "#This takes the input and turns it into a vector (group of values)\n",
        "model.add(Flatten())\n",
        "# A usual NN layer as talked in DL intro blog\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# The num_classes has to be there, as this is the last layer. DO NOT CHANGE\n",
        "# The Softmax function has been talked about in Intro to DL CODE\n",
        "model.add(Dense(num_classes,activation='softmax'))\n",
        "# We shall continue using Adaptive Moment Estimation (ADAM) as optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# To prevent fluctuations, we will train the whole dataset 50 times, validating with the test set.\n",
        "model.fit(x=x,y=y,epochs=50,verbose=2, validation_data=(x_test,y_test))\n",
        "# Evaluating the model on test set\n",
        "score = model.evaluate(x_test,y_test,verbose=0)\n",
        "print(\"The accuracy is \"+str(int(score[1]*100))+\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}